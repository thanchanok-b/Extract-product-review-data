{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "700f5e4b-a7ff-44f2-9782-3d53775c5b3e",
   "metadata": {},
   "source": [
    "# Shopee API Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08d25fa6-be25-4c23-8545-61a98cdca1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.edge.options import Options\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "\n",
    "# Set up Edge WebDriver\n",
    "def setup_webdriver():\n",
    "    edge_options = Options()\n",
    "    driver = webdriver.Edge(options=edge_options)\n",
    "\n",
    "    # Enable Network interception and set custom headers\n",
    "    try:\n",
    "        driver.execute_cdp_cmd('Network.enable', {})\n",
    "        driver.execute_cdp_cmd('Network.setExtraHTTPHeaders', {\n",
    "            'headers': {\n",
    "                'af-ac-enc-dat': 'null'\n",
    "            }\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"CDP command error: {e}\")\n",
    "\n",
    "    return driver\n",
    "\n",
    "\n",
    "# Function to wait for Shopee login\n",
    "def login_shopee(driver):\n",
    "    # Open a new tab to the Shopee login page\n",
    "    driver.execute_script(\"window.open('https://shopee.co.th/buyer/login', '_blank');\")\n",
    "    \n",
    "    # Switch to the login tab\n",
    "    driver.switch_to.window(driver.window_handles[1])\n",
    "    \n",
    "    # Wait for the user to manually log in\n",
    "    print(\"Please log in manually...\")\n",
    "    while True:\n",
    "        current_url = driver.current_url\n",
    "        # Check if login is successful\n",
    "        if \"verify/captcha?\" in current_url or \"home\" in current_url:  # Updated: change matching text in current_url from 'verify/traffic/error' to 'verify/captcha?'\n",
    "            print(\"Login successful and verified.\")\n",
    "            break\n",
    "        time.sleep(2)\n",
    "    \n",
    "    # Switch back to the product page tab\n",
    "    driver.switch_to.window(driver.window_handles[0])\n",
    "\n",
    "\n",
    "# Function to refresh the Shopee product page\n",
    "def refresh_page(driver):\n",
    "    driver.refresh()\n",
    "    print(\"Refreshed the product page.\")\n",
    "\n",
    "\n",
    "# Function to sanitize file names by removing invalid characters and limiting the length\n",
    "def sanitize_filename(filename, max_length=100):\n",
    "    sanitized = re.sub(r'[\\\\/*?:\"<>|]', \"\", filename)\n",
    "    return sanitized[:max_length]  # Limit filename length to the specified max_length\n",
    "\n",
    "\n",
    "# Function to check if the file exists and append a number if necessary\n",
    "def check_and_create_filename(sanitized_product_name):\n",
    "    base_filename = sanitized_product_name\n",
    "    counter = 1\n",
    "    output_filename = f\"{base_filename}.xlsx\"\n",
    "\n",
    "    # Check if file exists and modify name if needed\n",
    "    while os.path.isfile(output_filename):\n",
    "        output_filename = f\"{base_filename}({counter}).xlsx\"\n",
    "        counter += 1\n",
    "\n",
    "    return output_filename\n",
    "\n",
    "\n",
    "def scrape_ratings(driver, url):\n",
    "    # Define offset range and step\n",
    "    offset_start = 0\n",
    "    offset_end = 3000\n",
    "    offset_step = 50\n",
    "\n",
    "    # Use regex to extract shopid and itemid from the URL\n",
    "    r = re.search(r\"i\\.(\\d+)\\.(\\d+)\", url)\n",
    "    shopid, itemid = r[1], r[2]\n",
    "\n",
    "    base_url = f\"https://shopee.co.th/api/v2/item/get_ratings?filter=0&flag=1&itemid={itemid}&limit=50&offset={{offset}}&shopid={shopid}&type=0\"\n",
    "\n",
    "    # Create a dictionary to store the scraped data\n",
    "    d = {\n",
    "        \"username\": [],\n",
    "        \"rating\": [],\n",
    "        \"comment\": [],\n",
    "        \"date\": [],\n",
    "        \"product_selected\": [],\n",
    "        \"name\": [],\n",
    "        \"options\": [],\n",
    "    }\n",
    "\n",
    "    total_comments = 0\n",
    "    consecutive_empty_offsets = 0  # Counter for consecutive offsets with no data\n",
    "    max_empty_offsets = 3         # Maximum number of consecutive empty offsets allowed\n",
    "\n",
    "    # Scrape data from each offset page\n",
    "    for offset in range(offset_start, offset_end + 1, offset_step):\n",
    "        url = base_url.format(offset=offset)\n",
    "        # time.sleep(5)\n",
    "        driver.get(url)\n",
    "\n",
    "        # Fetch HTML and convert it to JSON\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "\n",
    "        try:\n",
    "            div_tag = soup.find(\"div\").text\n",
    "            data = json.loads(div_tag)\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing page source for offset {offset}: {e}\")\n",
    "            consecutive_empty_offsets += 1  # Increment empty counter for failed parse\n",
    "            if consecutive_empty_offsets >= max_empty_offsets:\n",
    "                # print(\"No data for consecutive offsets. Skipping to next URL.\")\n",
    "                break\n",
    "            continue\n",
    "\n",
    "        if \"data\" not in data or data[\"data\"] is None or \"ratings\" not in data[\"data\"] or not isinstance(data[\"data\"][\"ratings\"], list):\n",
    "            # print(f\"No data found for offset {offset}.\")\n",
    "            consecutive_empty_offsets += 1  # Increment empty counter for no data\n",
    "            if consecutive_empty_offsets >= max_empty_offsets:\n",
    "                # print(\"No data for consecutive offsets. Skipping to next URL.\")\n",
    "                break\n",
    "            continue\n",
    "\n",
    "        # Reset counter if data is found\n",
    "        consecutive_empty_offsets = 0\n",
    "\n",
    "        comments_in_page = len(data[\"data\"][\"ratings\"])\n",
    "        total_comments += comments_in_page\n",
    "        print(f\"Offset {offset}: {comments_in_page} comments\")\n",
    "\n",
    "        # Store scraped data in the dictionary\n",
    "        for rating in data[\"data\"][\"ratings\"]:\n",
    "            product_items = rating.get(\"product_items\", [])\n",
    "            d[\"username\"].append(rating.get(\"author_username\", np.nan))\n",
    "            d[\"rating\"].append(rating.get(\"rating_star\", np.nan))\n",
    "            d[\"comment\"].append(rating.get(\"comment\", \"\"))\n",
    "\n",
    "            # Convert timestamp to date\n",
    "            date = datetime.fromtimestamp(rating[\"ctime\"]).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            d[\"date\"].append(date)\n",
    "\n",
    "            if product_items:\n",
    "                d[\"product_selected\"].append(product_items)\n",
    "                d[\"name\"].append(product_items[0].get(\"name\", np.nan))\n",
    "                d[\"options\"].append(product_items[0].get(\"options\", np.nan))\n",
    "            else:\n",
    "                d[\"product_selected\"].append(None)\n",
    "                d[\"name\"].append(None)\n",
    "                d[\"options\"].append(None)\n",
    "\n",
    "    # Convert dictionary to DataFrame\n",
    "    if total_comments > 0:\n",
    "        df = pd.DataFrame(d)\n",
    "        print(f\"Fetched all {total_comments} comments\")\n",
    "        return df\n",
    "    else:\n",
    "        print(f\"No comments found for URL: {url}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192e9ccd-a15e-4674-847b-2c5b4ba0d380",
   "metadata": {},
   "source": [
    "####  scraping comments from many products, create correponding df1,df2,df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c953bee1-0fad-4294-958c-4cd5a0d8535e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please log in manually...\n",
      "Login successful and verified.\n",
      "Processing URL: https://shopee.co.th/MizuMi-UV-Water-Serum-SPF50-PA-8g-No.1-Best-Selling-Sunscreen-%E0%B9%80%E0%B8%8B%E0%B8%A3%E0%B8%B1%E0%B9%88%E0%B8%A1%E0%B8%81%E0%B8%B1%E0%B8%99%E0%B9%81%E0%B8%94%E0%B8%94-%E0%B8%9A%E0%B8%B2%E0%B8%87%E0%B9%80%E0%B8%9A%E0%B8%B2-%E0%B8%8B%E0%B8%B6%E0%B8%A1%E0%B9%84%E0%B8%A7-%E0%B9%84%E0%B8%A1%E0%B9%88%E0%B8%AD%E0%B8%B8%E0%B8%94%E0%B8%95%E0%B8%B1%E0%B8%99-%E0%B8%9B%E0%B8%81%E0%B8%9B%E0%B9%89%E0%B8%AD%E0%B8%87%E0%B8%AA%E0%B8%B9%E0%B8%87%E0%B8%AA%E0%B8%B8%E0%B8%94-i.70802054.1208714831?sp_atk=80074ea6-49b8-4581-8d32-650693a36aae\n",
      "Error parsing page source for offset 0: Expecting value: line 1 column 1 (char 0)\n",
      "Error parsing page source for offset 50: Expecting value: line 1 column 1 (char 0)\n",
      "Error parsing page source for offset 100: Expecting value: line 1 column 1 (char 0)\n",
      "No comments found for URL: https://shopee.co.th/api/v2/item/get_ratings?filter=0&flag=1&itemid=1208714831&limit=50&offset=100&shopid=70802054&type=0\n",
      "No comments found for URL: https://shopee.co.th/MizuMi-UV-Water-Serum-SPF50-PA-8g-No.1-Best-Selling-Sunscreen-%E0%B9%80%E0%B8%8B%E0%B8%A3%E0%B8%B1%E0%B9%88%E0%B8%A1%E0%B8%81%E0%B8%B1%E0%B8%99%E0%B9%81%E0%B8%94%E0%B8%94-%E0%B8%9A%E0%B8%B2%E0%B8%87%E0%B9%80%E0%B8%9A%E0%B8%B2-%E0%B8%8B%E0%B8%B6%E0%B8%A1%E0%B9%84%E0%B8%A7-%E0%B9%84%E0%B8%A1%E0%B9%88%E0%B8%AD%E0%B8%B8%E0%B8%94%E0%B8%95%E0%B8%B1%E0%B8%99-%E0%B8%9B%E0%B8%81%E0%B8%9B%E0%B9%89%E0%B8%AD%E0%B8%87%E0%B8%AA%E0%B8%B9%E0%B8%87%E0%B8%AA%E0%B8%B8%E0%B8%94-i.70802054.1208714831?sp_atk=80074ea6-49b8-4581-8d32-650693a36aae. Skipping to next URL.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "driver = setup_webdriver()\n",
    "login_shopee(driver)\n",
    "\n",
    "url_list = [\n",
    "    \"https://shopee.co.th/MizuMi-UV-Water-Serum-SPF50-PA-8g-No.1-Best-Selling-Sunscreen-%E0%B9%80%E0%B8%8B%E0%B8%A3%E0%B8%B1%E0%B9%88%E0%B8%A1%E0%B8%81%E0%B8%B1%E0%B8%99%E0%B9%81%E0%B8%94%E0%B8%94-%E0%B8%9A%E0%B8%B2%E0%B8%87%E0%B9%80%E0%B8%9A%E0%B8%B2-%E0%B8%8B%E0%B8%B6%E0%B8%A1%E0%B9%84%E0%B8%A7-%E0%B9%84%E0%B8%A1%E0%B9%88%E0%B8%AD%E0%B8%B8%E0%B8%94%E0%B8%95%E0%B8%B1%E0%B8%99-%E0%B8%9B%E0%B8%81%E0%B8%9B%E0%B9%89%E0%B8%AD%E0%B8%87%E0%B8%AA%E0%B8%B9%E0%B8%87%E0%B8%AA%E0%B8%B8%E0%B8%94-i.70802054.1208714831?sp_atk=80074ea6-49b8-4581-8d32-650693a36aae\"\n",
    "]\n",
    "\n",
    "# Scrape data from all URLs in the list\n",
    "for url in url_list:\n",
    "    print(f\"Processing URL: {url}\")\n",
    "    df = scrape_ratings(driver, url)\n",
    "\n",
    "    # ตรวจสอบว่า DataFrame มีข้อมูลหรือไม่\n",
    "    if df.empty:\n",
    "        print(f\"No comments found for URL: {url}. Skipping to next URL.\")\n",
    "        print(\"-\" * 50)\n",
    "        continue\n",
    "\n",
    "    # ลบคอมเมนต์ที่ว่าง\n",
    "    df[\"comment\"] = df[\"comment\"].replace(\"\", np.nan)\n",
    "    df.dropna(subset=[\"comment\"], inplace=True)\n",
    "\n",
    "    # ลบข้อมูลที่ซ้ำกัน\n",
    "    df.drop_duplicates(subset=[\"username\", \"comment\"], inplace=True)\n",
    "\n",
    "    # ดึงชื่อผลิตภัณฑ์จากแถวแรก\n",
    "    product_name = df[\"name\"].iloc[0] if not df.empty else \"Unknown Product\"\n",
    "\n",
    "    # แปลงชื่อไฟล์ให้ปลอดภัยและไม่ยาวเกินไป\n",
    "    sanitized_product_name = sanitize_filename(product_name)\n",
    "\n",
    "    # สร้างชื่อไฟล์ที่ไม่ซ้ำกัน\n",
    "    output_filename = check_and_create_filename(sanitized_product_name)\n",
    "\n",
    "    # บันทึก DataFrame ลงไฟล์ Excel\n",
    "    df.to_excel(output_filename, index=False)\n",
    "\n",
    "    # แสดงข้อความยืนยัน\n",
    "    print(f\"Saved: {output_filename}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Quit the driver\n",
    "# driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baa892e-878d-4b7b-a205-fb3395fd3073",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f382ee5-8a11-4371-ad95-73e2b0dc6e14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
